---
title: "Human Activity Recognition by Machine Learning"
output: html_document
---

## Overview
In this project, using the data measured from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, we use machine learning to recognize the activities (correct and incorrect barbell lifts in 5 different ways) performed by the test data. 

## Download data
```{r, cache= TRUE}
fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(fileUrl, destfile = "train.csv", method = "curl")
trainData <- read.csv("train.csv")

fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileUrl, destfile = "test.csv", method = "curl")
testData <- read.csv("test.csv")
```
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

## Choosing predictors
```{r}
library(dplyr)
library(data.table)
trainDT <- data.table(trainData)
train <- select(trainDT, c(num_window:total_accel_belt, gyros_belt_x:total_accel_arm,gyros_arm_x:magnet_arm_z,roll_dumbbell: yaw_dumbbell,total_accel_dumbbell,gyros_dumbbell_x:yaw_forearm,total_accel_forearm, gyros_forearm_x:classe))

testDT <- data.table(testData)
test <- select(testDT, c(num_window:total_accel_belt, gyros_belt_x:total_accel_arm,gyros_arm_x:magnet_arm_z,roll_dumbbell: yaw_dumbbell,total_accel_dumbbell,gyros_dumbbell_x:yaw_forearm,total_accel_forearm, gyros_forearm_x:magnet_forearm_z))
```
We discard the variables whose values are largely NA. Moreover, we discard variables user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, and new_window.

## Dividing the training data into six parts 
```{r}
library(caret)
```

```{r}
set.seed(71)
inTrain1 <- createDataPartition(y = train$classe, p=1/6, list=FALSE)
Train1 <- filter(train,inTrain1)
train_other1 <- filter(train,-inTrain1)
set.seed(71)
inTrain2 <- createDataPartition(y = train_other1$classe, p=1/5, list=FALSE)
Train2 <- filter(train_other1,inTrain2)
train_other2 <- filter(train_other1,-inTrain2)
set.seed(71)
inTrain3 <- createDataPartition(y = train_other2$classe, p=1/5, list=FALSE)
Train3 <- filter(train_other2,inTrain3)
train_other3 <- filter(train_other2,-inTrain3)
set.seed(71)
inTrain4 <- createDataPartition(y = train_other3$classe, p=1/5, list=FALSE)
Train4 <- filter(train_other3,inTrain4)
train_other4 <- filter(train_other3,-inTrain4)
set.seed(71)
inTrain5 <- createDataPartition(y = train_other4$classe, p=1/2, list=FALSE)
Train5 <- filter(train_other4,inTrain5)
Train6 <- filter(train_other4,-inTrain5)
```
We randomly divide the training data into six parts of roughly equal size. One reason is that the total number of observations in our original training set is 19622, for which it will take too long to train the model. The other reason is that we can compare the different predictions made by the six training data subsets.

## Prediction by random forest
In the following, for each training data subset, we proceed in the following steps:

1. Divide the subset further into two parts: A randomly chosen 80% to be used for training, and the remaining 20% for cross validation.
2. Train the random forest model using the chosen training set from step 1.
3. Make prediction on the cross validation set using the model obtained from step 2. Create confusion matrix.
4. Make prediction on the test data set.

### model training 1

```{r}
set.seed(71)
part1 <- createDataPartition(y = Train1$classe, p=0.8, list=FALSE)
tr1 <- filter(Train1,part1)
cv1 <- filter(Train1,-part1)
```

```{r,cache= TRUE}
mod1 <- train(classe ~.,method="rf", data=tr1)
```

```{r}
pred1 <- predict(mod1, cv1)
print(confusionMatrix(pred1, cv1$classe), digits=4)
```

```{r}
predict(mod1, test)
```

### model training 2

```{r}
set.seed(71)
part2 <- createDataPartition(y = Train2$classe, p=0.8, list=FALSE)
tr2 <- filter(Train2,part2)
cv2 <- filter(Train2,-part2)
```

```{r,cache= TRUE}
mod2 <- train(classe ~.,method="rf", data=tr2)
```

```{r}
pred2 <- predict(mod2, cv2)
print(confusionMatrix(pred2, cv2$classe), digits=4)
```

```{r}
predict(mod2, test)
```


### model training 3

```{r}
set.seed(71)
part3 <- createDataPartition(y = Train3$classe, p=0.8, list=FALSE)
tr3 <- filter(Train3,part3)
cv3 <- filter(Train3,-part3)
```

```{r,cache= TRUE}
mod3 <- train(classe ~.,method="rf", data=tr3)
```

```{r}
pred3 <- predict(mod3, cv3)
print(confusionMatrix(pred3, cv3$classe), digits=4)
```

```{r}
predict(mod3, test)
```

### model training 4

```{r}
set.seed(71)
part4 <- createDataPartition(y = Train4$classe, p=0.8, list=FALSE)
tr4 <- filter(Train4,part4)
cv4 <- filter(Train4,-part4)
```

```{r,cache= TRUE}
mod4 <- train(classe ~.,method="rf", data=tr4)
```

```{r}
pred4 <- predict(mod4, cv4)
print(confusionMatrix(pred4, cv4$classe), digits=4)
```

```{r}
predict(mod4, test)
```

### model training 5

```{r}
set.seed(71)
part5 <- createDataPartition(y = Train5$classe, p=0.8, list=FALSE)
tr5 <- filter(Train5,part5)
cv5 <- filter(Train5,-part5)
```

```{r,cache= TRUE}
mod5 <- train(classe ~.,method="rf", data=tr5)
```

```{r}
pred5 <- predict(mod5, cv5)
print(confusionMatrix(pred5, cv5$classe), digits=4)
```

```{r}
predict(mod5, test)
```

### model training 6

```{r}
set.seed(71)
part6 <- createDataPartition(y = Train6$classe, p=0.8, list=FALSE)
tr6 <- filter(Train6,part6)
cv6 <- filter(Train6,-part6)
```

```{r,cache= TRUE}
mod6 <- train(classe ~.,method="rf", data=tr6)
```

```{r}
pred6 <- predict(mod6, cv6)
print(confusionMatrix(pred6, cv6$classe), digits=4)
```

```{r}
predict(mod6, test)
```

## Out of sample error

Since the cross validation data sets are not used to train the models, out of sample errors can be obtained by comparing the predictions the model make on the cross validation data and the actual activities they correspond.

Reading the 'Accuracy' value below each confusion matrix, we compute that the out of sample errors for the six model trainings are 

1. model training 1: 1 - 0.9801 = `r 1 - 0.9801`,
2. model training 2: 1 - 0.9755 = `r 1- 0.9755`, 
3. model training 3: 1 - 0.9559 = `r 1 - 0.9559`, 
4. model training 4: 1 - 0.9928 = `r 1- 0.9928`, 
5. model training 5: 1 - 0.9892 = `r 1 - 0.9892`, 
6. model training 6: 1 - 0.9727 = `r 1- 0.9727`. 

The average is `r 0.0223`.

## Prediction

The six predictions are the same for all of the 20 test cases except for the third and the eighth cases. The predictions made by the two models with the highest accuracy on the cross validation sets agree on these two cases, and they are

B A B A A E D B A A B C B A E E A B B B.